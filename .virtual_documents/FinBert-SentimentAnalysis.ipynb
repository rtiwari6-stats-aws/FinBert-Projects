get_ipython().run_line_magic("pip", " install transformers")


import pandas as pd
import scipy
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification


data = pd.read_csv('all-data.csv', 
                   encoding='unicode_escape',
                   names=['Sentiment', 'Text'])
data.head()


# Load the FinBERT model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")

tokenizer_kwargs = {"padding": True, "truncation": True, "max_length": 512}

# Define a function to predict sentiment
def predict_sentiment(text):
    encoded_text = tokenizer(text, return_tensors="pt", **tokenizer_kwargs)
    # Get predictions from the model
    with torch.no_grad():
        logits = model(**encoded_text).logits
    scores = {k: v for k, v in zip(model.config.id2label.values(), scipy.special.softmax(logits.numpy().squeeze()))}
    print(scores)
    print(f"Predicted sentiment for '{text}': {max(scores, key=scores.get)}")


test_text = "The company's stock price is not expected to surge in the coming months."
predict_sentiment(test_text)


test_text = "The company's stock price is going down."
predict_sentiment(test_text)

test_text = "The british pound weakened but stocks rallied."
predict_sentiment(test_text)



preds = []
preds_proba = []
text=data['Text'].tolist()
tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
tokenizer_kwargs = {"padding": True, "truncation": True, "max_length": 512}
for x in text:
    # disable gradient calculation for efficiency since we're not performing backpropagation during prediction.
    with torch.no_grad():
        # This line uses the tokenizer to convert the current text x into a format suitable for the model. 
        # The return_tensors="pt" argument specifies converting the data to PyTorch tensors, 
        # and the **tokenizer_kwargs dictionary unpacks additional arguments like 
        # padding, truncation, and maximum sequence length (set to 512 here).
        input_sequence = tokenizer(x, return_tensors="pt", **tokenizer_kwargs)
        # These logits represent the raw scores for each possible sentiment class.
        logits = model(**input_sequence).logits
        # create a dictionary to store the predicted sentiment label and its corresponding probability score
        scores = {
        k: v
        # iterates through the model's configuration (model.config.id2label.values()) to get the sentiment labels and 
        # combines them with the softmax probabilities calculated using scipy.special.softmax on the logits 
        # converted to NumPy array and squeezed to remove extra dimensions
        for k, v in zip(
            model.config.id2label.values(),
            scipy.special.softmax(logits.numpy().squeeze()),
        )
    }
    
    # This line finds the sentiment label (key) in the scores dictionary with the highest probability value.
    sentimentFinbert = max(scores, key=scores.get)
    
    # This captures the highest probability score (value) from the scores dictionary.
    probabilityFinbert = max(scores.values())
    preds.append(sentimentFinbert)
    preds_proba.append(probabilityFinbert)


y = data['Sentiment'].to_list()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score

print(f'Accuracy-Score: {accuracy_score(y, preds)}')




